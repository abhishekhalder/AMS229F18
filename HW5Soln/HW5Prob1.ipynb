{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-cut problem\n",
    "\n",
    "An undirected graph $\\mathscr{G} = (\\mathcal{V},\\mathcal{E})$ is defined as a collection of two sets: a set of vertices $\\mathcal{V}$, and a set of edges $\\mathcal{E}$. If there is an edge $e_{ij}\\in\\mathcal{E}$ connecting the vertices $v_{i},v_{j}\\in\\mathcal{V}$, then we can assign an weight $w_{ij} \\geq 0$ to that edge to model the relative importance of that particular edge. Assuming the cardinality of the vertex set $|\\mathcal{V}|=n$ , we normalize the weights as $\\sum_{i\\leq j}w_{ij} = 1$. This gives rise to a weighted directed graph $\\mathscr{G}_{W} = (\\mathcal{V},\\mathcal{E},W)$ where the edge weight matrix $W\\in\\mathbb{R}^{n\\times n}$ is symmetric with elements in $[0,1]$.\n",
    "\n",
    "A \"cut\" in $\\mathscr{G}_{W}$ is simply a partition of the vertex set $\\mathcal{V}$ into two (disjoint) sets: $\\mathcal{S}$ and its complement $\\overline{\\mathcal{S}}:=\\mathcal{V}\\setminus\\mathcal{S}$, that is, $\\mathcal{V}=\\mathcal{S}\\cup\\overline{\\mathcal{S}}$. The \"weight of a cut\" is the sum of weights for those (and only those) edges which connect vertices in $\\mathcal{S}$ to vertices in $\\overline{\\mathcal{S}}$. The \"max-cut problem\" concerns with finding a cut in $\\mathscr{G}_{W}$ that maximizes the weight of cut. \n",
    "\n",
    "In the following example graph with $n=4$ vertices, the \"red solid\" cut is the max-cut (cut weight = 0.7), while the \"blue dashed\" cut is sub-optimal (cut-weight = 0.5). In this example, the red max-cut corresponds to the partition $\\mathcal{V} = \\{v_{1},v_{3}\\} \\cup \\{v_{2},v_{4}\\}$. The sub-optimal blue cut corresponds to the partition $\\mathcal{V} = \\{v_{1},v_{2}\\} \\cup \\{v_{3},v_{4}\\}$.\n",
    "\n",
    "<img width=\"450\" src=\"maxcut.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (5+5=10 points) To formulate the max-cut problem as an optimization problem, let us consider any cut $\\mathcal{V}=\\mathcal{S}\\cup\\overline{\\mathcal{S}}$, and for $i=1,...,n$, assign a variable $x_{i}$ to each vertex of $\\mathscr{G}_{W} = (\\mathcal{V},\\mathcal{E},W)$, as\n",
    "\n",
    "$$x_{i} = \\begin{cases} \n",
    "1 & \\text{if}\\quad v_{i}\\in\\mathcal{S},\\\\\n",
    "-1 & \\text{if}\\quad v_{i}\\in\\overline{\\mathcal{S}}.\n",
    "\\end{cases}$$\n",
    "\n",
    "Next, notice that $(1-x_{i}x_{j}) = 0$ if the vertices $v_{i}$ and $v_{j}$ are in the same set, and $=2$ otherwise. Therefore, the quantity $\\sum_{i<j}w_{ij}(1-x_{i}x_{j})$ is twice the cut weight, and hence the max-cut problem is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "p^{*} := \\underset{x\\in\\{-1,+1\\}^{n}}{\\text{maximize}}\\quad\\displaystyle\\frac{1}{2}\\displaystyle\\sum_{i<j}w_{ij}\\left(1-x_{i}x_{j}\\right).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Explain why the above max-cut problem is equivalent to solving\n",
    "\n",
    "$$\\underset{x\\in\\{-1,+1\\}^{n}}{\\text{minimize}}\\quad x^{\\top}W x.$$\n",
    "\n",
    "Is this optimization problem convex? Why/why not? \n",
    "\n",
    "(Hint: $\\frac{1}{2}\\sum_{i<j}w_{ij}(1-x_{i}x_{j}) = \\frac{1}{4}\\sum_{i,j}w_{ij}(1-x_{i}x_{j})$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution: \n",
    "Since the diagonals (self-loops) do not contribute in determining max-cut, utilizing the hint, we have\n",
    "$$\\begin{aligned}\n",
    "p^{*} := \\underset{x\\in\\{-1,+1\\}^{n}}{\\text{maximize}}\\quad\\displaystyle\\frac{1}{4}\\sum_{i,j}w_{ij}(1-x_{i}x_{j}) = \\underset{x\\in\\{-1,+1\\}^{n}}{\\text{maximize}}\\quad\\bigg\\{\\frac{1}{4}\\left(\\sum_{i,j}w_{ij}\\right)-\\frac{1}{4}x^{\\top}Wx\\bigg\\}.\n",
    "\\end{aligned}$$\n",
    "Since $\\sum_{i,j}w_{ij}$ is constant, hence computing $p^{*}$ redces to minimizing $x^{\\top}Wx$.\n",
    "\n",
    "Computing $p^{*}$ is a non-convex problem since the constraint set $x\\in\\{-1,1\\}^{n}$ is non-convex (union of vertices of $n$-dimensional hypercube). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (15 points) Although the constraint set of the optimization problem derived in part (a) is finite, it has cardinality $2^{n}$, and therefore, it is computationally impractical to take enumerative approach to solve it for large $n$. In fact, this optimization problem is known to be NP hard.\n",
    "\n",
    "Derive the Lagrangian, the Lagrange dual function, and the dual optimization problem for the primal problem obtained in part (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Letting $\\nu\\in\\mathbb{R}^{n}$ be the Lagrange multiplier for the equality constraints $x_{i}^{2} = 1$, $i=1,...,n$, we can write the Lagrangian as \n",
    "\n",
    "$$L(x,\\nu) = x^{\\top}Wx + \\sum_{i=1}^{n}\\nu_{i}(x_{i}^{2} - 1) = x^{\\top}\\left(W + \\text{diag}(\\nu)\\right)x - 1^{\\top}\\nu.$$\n",
    "\n",
    "Therefore, the Lagrange dual function is\n",
    "\n",
    "$$g(\\nu) := \\inf_{x\\in\\mathbb{R}^{n}}\\:L(x,\\nu) = \\begin{cases}\n",
    "-1^{\\top}\\nu &\\text{if}\\quad W + \\text{diag}(\\nu) \\succeq 0,\\\\\n",
    "-\\infty &\\text{otherwise}.\n",
    "\\end{cases}$$\n",
    "\n",
    "Hence, the Lagrange dual optimization problem is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\delta^{*} = \\quad &\\underset{\\nu\\in\\mathbb{R}^{n}}{\\text{maximize}}\\quad -1^{\\top}\\nu\\\\\n",
    "&\\text{subject to} \\quad W + \\text{diag}(\\nu) \\succeq 0.\n",
    "\\end{aligned}$$\n",
    "\n",
    "By weak duality, $\\delta^{*} \\leq \\underset{x\\in\\{-1,+1\\}^{n}}{\\text{minimize}}\\quad x^{\\top}Wx \\quad \\Rightarrow p^{*} \\leq \\frac{1}{4}\\left(\\sum_{i,j}w_{ij} - \\delta^{*}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) (10 points) Use the Lagrange dual function derived in part (b) to provide a **simple sub-optimal** (meaning, not the tightest) bound for $p^{*}$ in part (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "Since $g(\\nu) \\leq \\pi^{*}$ for all $\\nu \\in \\mathbb{R}^{n}$, a specific (sub-optimal) choice for $\\nu$ is\n",
    "$$\\widetilde{\\nu} = -\\lambda_{\\min}(W)1,$$\n",
    "which is dual feasible since $W + \\text{diag}(\\widetilde{\\nu}) = W - \\lambda_{\\min}(W)I \\succeq 0$. For this particular choice, we get the bound \n",
    "\n",
    "$$\\delta^{*} \\geq g(\\widetilde{\\nu}) = \\min\\{0,n\\lambda_{\\min}(W)\\} \\quad \\Rightarrow \\quad p^{*} \\leq \\frac{1}{4}\\bigg\\{\\left(\\sum_{i,j}w_{ij}\\right) - \\min\\left(0,n\\lambda_{\\min}(W)\\right)\\bigg\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) (10 points) Is the dual problem derived in part (b) a convex optimization problem? If \"yes\", then what kind of convex optimization problem is it? If \"no\", then explain why not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "Yes, it is a convex optimization problem (dual problems are always convex).\n",
    "\n",
    "The dual problem derived in part (b) is an SDP since it involves minimizing a linear objective subject to an LMI constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) (15+5=20 points) For the example graph with 4 vertices given above, write a code using cvxpy, to solve the dual problem. You need to write the code in your solution notebook. Use the answer of your cvxpy code to write an inequality for $p^{*}$ in part (a).\n",
    "\n",
    "(Hint: Weak duality.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('From simple suboptimal bound in part (c):  p* <=', 0.7310219833361814)\n",
      "('From dual solution via CVXPY:   p* <=', 0.70001141875788664)\n",
      "True solution (as explained pictorially before part (a)): p* = 0.7\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "\n",
    "W = np.array([[0.1, 0.2, 0,   0.3],\n",
    "              [0.2, 0,   0.2, 0],\n",
    "              [0,   0.2, 0,   0],\n",
    "              [0.3, 0,   0,   0.2]])\n",
    "\n",
    "# dual variable\n",
    "nu = cvx.Variable(4,1)\n",
    "# dual optimization problem\n",
    "obj = cvx.Maximize(-cvx.sum_entries(nu))\n",
    "constraints = [W + cvx.diag(nu) >> 0]\n",
    "\n",
    "prob = cvx.Problem(obj, constraints)\n",
    "prob.solve()\n",
    "\n",
    "p_subopt = (W.sum() - min(0,4*la.eigvalsh(W).min()))/4\n",
    "\n",
    "print('From simple suboptimal bound in part (c):  p* <=', p_subopt)\n",
    "print('From dual solution via CVXPY:   p* <=', (W.sum() + np.sum(nu.value))/4)\n",
    "print('True solution (as explained pictorially before part (a)): p* = 0.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(f) (5+5+10=20 points) Let us now go back to the problem derived in part (a):\n",
    "\n",
    "$$\\underset{x\\in\\{-1,+1\\}^{n}}{\\text{minimize}}\\quad x^{\\top}W x.$$\n",
    "\n",
    "Prove that the above optimization problem can be re-written as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\underset{X\\in\\mathbb{S}^{n}_{+}}{\\text{minimize}}\\quad \\text{trace}\\left(WX\\right)\\\\\n",
    "&\\text{subject to}\\quad X_{ii} = 1,\\quad i=1,...,n,\\\\\n",
    "& \\qquad\\qquad\\; \\text{rank}(X) = 1.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Is the above re-written optimization problem convex? Why/why not?\n",
    "\n",
    "Consider yet another modification of the above problem, obtained by ignoring/deleting the rank constraint:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q^{*} =\\, &\\underset{X\\in\\mathbb{S}^{n}_{+}}{\\text{minimize}}\\quad \\text{trace}\\left(WX\\right)\\\\\n",
    "&\\text{subject to}\\quad X_{ii} = 1,\\quad i=1,...,n.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Is the above optimization problem convex? Why/why not? Write an inequality between $p^{*}$ in part (a) and $q^{*}$ given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "Writing the objective $x^{\\top}Wx = \\text{trace}\\left(x^{\\top}Wx\\right) = \\text{trace}\\left(WX\\right)$, where $X:=xx^{\\top}$ is an outer-product matrix, we get the desired optimization problem. Notice that the constraints $\\text{rank}(X)=1$ and that $X\\succeq 0$ result form $X$ being an outer-product matrix, while the constraint $X_{ii}=1$ results from the condition $x_{i}^{2} = 1$, which is specific to this problem.\n",
    "\n",
    "The re-written optimization problem is nonconvex due to the rank constraint.\n",
    "\n",
    "Yes, ignoring/deleting the rank constraint makes the resulting problem convex. This is because the problem then reduces to minimizing a linear objective subject to linear constraints over the positive semidefinite cone.\n",
    "\n",
    "Since $q^{*}$ is obtained by a relaxation of the problem of computing $p^{*}$, hence it yields the bound\n",
    "\n",
    "$$p^{*} \\leq \\frac{1}{4}\\left(\\sum_{i,j}w_{ij} - q^{*}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) (5+5=10 points) Let us examine the geometric meaning of the constraint set for $q^{*}$ in part (f). This set is called **elliptope**. For $n=3$, the elliptope\n",
    "\n",
    "$$\\{X \\in \\mathbb{S}^{3}_{+} \\:\\mid\\: X_{11} = X_{22} = X_{33} = 1\\} = \\Bigg\\{(x,y,z)\\in\\mathbb{R}^{3} \\:\\mid\\: \\begin{bmatrix} 1 & x & y\\\\\n",
    "x & 1 & z\\\\\n",
    "y & z & 1\\end{bmatrix} \\succeq 0\n",
    "\\Bigg\\}$$\n",
    "can be visualized as a subset of $\\mathbb{R}^{3}$. Write a code to make a 3D plot the above set superimposed with the cube $[-1,1]^{3}$.\n",
    "\n",
    "Is the above 3D set convex? Is it contained in $[-1,1]^{3}$? Mathematically justify your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "To plot the set, we first note that the positive definiteness of the matrix $\\begin{bmatrix} 1 & x & y\\\\\n",
    "x & 1 & z\\\\\n",
    "y & z & 1\\end{bmatrix}$ is equivalent to requiring its all principal (not just leading principal) minors $\\geq 0$ (see e.g., Lecture 9, p. 11). This gives us the following 7 polynomial inequalities:\n",
    "\n",
    "$$1 \\geq 0\\;\\text{(thrice)},\\quad 1-x^{2}\\geq 0, \\quad 1-y^{2}\\geq 0, \\quad 1-z^{2}\\geq 0, \\quad 1 + 2xyz - x^{2} - y^{2} - z^{2} \\geq 0.$$\n",
    "\n",
    "Three of the above seven inequalities are trivial ($1\\geq 0$), three are quadratic inequalities, and one is cubic inequality. We rewrite the cubic inequality as a quadratic inequality in $z$ as function of $(x,y)$, given by $z(x,y):= z^{2} - 2xyz + (x^{2}+y^{2}-1)\\leq 0$. Then, we can plot the boundary of the elliptope surface by plotting $z(x,y)=0$, i.e., $z_{\\pm} = -xy \\pm \\sqrt{x^{2}y^{2} - x^{2} - y^{2} + 1}$. The wireframe plot of the elliptope, thus generated, is shown below. The Python code to generate this plot is elliptope.py (in the notebook repository)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"450\" src=\"elliptope.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elliptope is a convex set because it is obtained as an intersection of an LMI (a convex constraint) with linear equality constraint $X_{ii}=1$.\n",
    "\n",
    "The elliptope is contained in the cube $[-1,1]^{3}$ since 3 of the 7 inequalities defining the elliptope are: $1-x^{2}\\geq 0, \\quad 1-y^{2}\\geq 0, \\quad 1-z^{2}\\geq 0$, or equivalently, $-1\\leq x,y,z\\leq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(h) (5 points) Write and explain the relation between $p^{*}$ (optimal value of the primal problem), $d^{*}$ (optimal value of the dual problem), and $q^{*}$ (optimal value of the problem derived in part (f)).\n",
    "\n",
    "(Hint: Consider the dual problem of the dual problem derived in part (b).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Solution:\n",
    "From part (f), we know \n",
    "$$p^{*} \\leq \\frac{1}{4}\\left(\\sum_{i,j}w_{ij} - q^{*}\\right).$$ \n",
    "Since $d^{*}$ is related to $\\delta^{*}$ by an additive constant $\\sum_{i,j}w_{ij}$, we can simply relate $p^{*}$ with $\\delta^{*}$, which we already did in part (b) as\n",
    "$$p^{*} \\leq \\frac{1}{4}\\left(\\sum_{i,j}w_{ij} - \\delta^{*}\\right).$$\n",
    "So far, it is not obvious how $q^{*}$ compare with $\\delta^{*}$. To resolve that we follow the hint and consider the dual of dual (bi-dual) optimization problem derived in part (b). The dual problem derived in part (b) is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\underset{\\nu\\in\\mathbb{R}^{n}}{\\text{minimize}}\\quad 1^{\\top}\\nu\\\\\n",
    "&\\text{subject to} \\quad -\\left(W + \\text{diag}(\\nu)\\right) \\preceq 0.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Since $1^{\\top}\\nu = \\text{trace(diag}(\\nu))$, hence its Lagrangian $L(\\nu,Z) = \\text{trace}((I-Z)\\text{diag}(\\nu) - ZW)$, $Z\\in\\mathbb{S}^{n}_{+}$, results the dual function\n",
    "\n",
    "$$g(Z) = \\underset{\\nu\\in\\mathbb{R}^{n}}{\\min}L(\\nu,Z) = \\begin{cases}\n",
    "-\\text{trace}(ZW) & \\text{if}\\quad Z_{ii}=1\\;\\forall i=1,...,n,\\\\\n",
    "-\\infty & \\text{otherwise.}\n",
    "\\end{cases}$$\n",
    "\n",
    "Therefore, the bi-dual optimization problem becomes: $\\underset{Z\\in\\mathbb{S}^{n}_{+}}{\\max}-\\text{trace}(ZW)$ subject to $Z_{ii}=1$ for all $i=1,...,n$, which is exactly the convex relaxation obtained in part (f). Therefore, $\\delta^{*} = q^{*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
